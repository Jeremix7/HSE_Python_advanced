{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed08d694",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05315efa",
   "metadata": {},
   "source": [
    "Most traditional machine learning algorithms work with structured data(numbers, categories, boolean data). But how to work with sequences? We can vectorize this sequence: cut the sequence into pieces and increment the value on the corresponding index in the vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc8fdb9",
   "metadata": {},
   "source": [
    "# Assignment Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e082a8",
   "metadata": {},
   "source": [
    "In this task, you need to write a class for vectorizing strings. The work of the class will be tested on genomic chains.\n",
    "\n",
    "An important parameter is ngram_size. It is responsible for what size we need to take the pieces of the string, will call them tokens. For ngram_size = 4: '​TTTCTGCCA' → ['TTTC', 'TTCT', 'TCTG', 'CTGC', 'TGCC', 'GCCA']. To encode tokens into indexes, first sort them in lexicographic order, and then give integer indexes starting from zero in the same order. Tokens can appear more than once in one row, so we will count the total number of each token in this row. The list of strings is usually called a corpus. The dictionary for converting tokens to indexes is built for the entire corpus. Example of transformation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec73e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size = 2\n",
    "\n",
    "corpus = [\n",
    "    'AATACAT',  # 'AA', 'AT', 'TA', 'AC', 'CA', 'AT'\n",
    "    'CTACCCT',  # 'CT', 'TA', 'AC', 'CC', 'CC', 'CT'\n",
    "    'TACCTAC',  # 'TA', 'AC', 'CC', 'CT', 'TA', 'AC'\n",
    "]\n",
    "\n",
    "vocab = {'AA': 0, 'AC': 1, 'AT': 2, 'CA': 3, 'CC': 4, 'CT': 5, 'TA': 6}\n",
    "\n",
    "transformed_corpus = [\n",
    "    [1, 1, 2, 1, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 2, 2, 1],\n",
    "    [0, 2, 0, 0, 1, 1, 2],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47d9cc",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a409a",
   "metadata": {},
   "source": [
    "Implement class CountVectorizer:\n",
    "- f​it  -  build a dictionary \"token to index\" from the input corpus and save it as an attribute of the class;\n",
    "- transform  -  transform a new corpus based on a saved dictionary, should return a list of lists. If some token from the new corpus is not represented in the dictionary, then you need to ignore it;\n",
    "- f​it_transform  -  fit and transform on the same corpus, should return a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef78d05e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T16:56:58.563683Z",
     "start_time": "2023-11-28T16:56:58.551210Z"
    }
   },
   "outputs": [],
   "source": [
    "class CountVectorizer:\n",
    "    def __init__(self, ngram_size):\n",
    "        self.ngram_size = ngram_size\n",
    "\n",
    "    def __matrix_parameters(self, corpus):\n",
    "        base_matrix = []\n",
    "        uniq_tokens = set()\n",
    "        for string in corpus:\n",
    "            start = 0\n",
    "            end = self.ngram_size\n",
    "            submatrix = []\n",
    "            for i in range(len(string) - self.ngram_size + 1):\n",
    "                submatrix.append(string[start:end])\n",
    "                uniq_tokens.add(string[start:end])\n",
    "                start += 1\n",
    "                end += 1\n",
    "            base_matrix.append(submatrix)\n",
    "        return uniq_tokens, base_matrix\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        uniq_tokens = self.__matrix_parameters(corpus)[0]\n",
    "        setattr(CountVectorizer, 'vocab',\n",
    "                {k: v for v, k in enumerate(sorted(uniq_tokens))})\n",
    "        return None\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        base_matrix = self.__matrix_parameters(corpus)[1]\n",
    "        return [[string.count(token) for token in CountVectorizer.vocab]\n",
    "                for string in base_matrix]\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        self.fit(corpus)\n",
    "        return self.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa439c",
   "metadata": {},
   "source": [
    "Check solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3dce7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'AATACAT',  # 'AA', 'AT', 'TA', 'AC', 'CA', 'AT'\n",
    "    'CTACCCT',  # 'CT', 'TA', 'AC', 'CC', 'CC', 'CT'\n",
    "    'TACCTAC',  # 'TA', 'AC', 'CC', 'CT', 'TA', 'AC'\n",
    "]\n",
    "\n",
    "correct_transformation = [\n",
    "    [1, 1, 2, 1, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 2, 2, 1],\n",
    "    [0, 2, 0, 0, 1, 1, 2],\n",
    "]\n",
    "\n",
    "# case 1\n",
    "vectorizer = CountVectorizer(ngram_size=2)\n",
    "vectorizer.fit(corpus)\n",
    "vectorizer.transform(corpus) == correct_transformation  # True\n",
    "\n",
    "# case 2\n",
    "vectorizer = CountVectorizer(ngram_size=2)\n",
    "vectorizer.fit_transform(corpus) == correct_transformation  # True\n",
    "\n",
    "# case 3\n",
    "corpus_2 = ['TCAATCAC', 'GGGGGGGGGGG', 'AAAA']\n",
    "vectorizer = CountVectorizer(ngram_size=2)\n",
    "vectorizer.fit(corpus)\n",
    "vectorizer.transform(corpus_2) == [\n",
    "    [1, 1, 1, 2, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [3, 0, 0, 0, 0, 0, 0]\n",
    "]  # True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
